{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T01:08:35.028023Z","iopub.execute_input":"2025-08-25T01:08:35.028247Z","iopub.status.idle":"2025-08-25T01:08:35.437841Z","shell.execute_reply.started":"2025-08-25T01:08:35.028224Z","shell.execute_reply":"2025-08-25T01:08:35.437068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_dev.tsv\n!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_dev_test.tsv\n!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_train.tsv\n!pip install transformers\n!pip install datasets\n!pip install evaluate\n!pip install torch\n!pip install scikit-learn\n# !pip install --upgrade accelerate\nimport logging\nimport os\nimport random\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport pandas as pd\nimport datasets\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport torch\nfrom sklearn.model_selection import StratifiedKFold\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    HfArgumentParser,\n    PretrainedConfig,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.utils import check_min_version, send_example_telemetry\nfrom transformers.utils.versions import require_version\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n\nlogger = logging.getLogger(__name__)\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\ntrain_file = 'blp25_hatespeech_subtask_1C_train.tsv'\nvalidation_file = 'blp25_hatespeech_subtask_1C_dev.tsv'\ntest_file = 'blp25_hatespeech_subtask_1C_dev_test.tsv'\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ntraining_args = TrainingArguments(\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    output_dir=\"./distilBERT_m/\",\n    overwrite_output_dir=True,\n    remove_unused_columns=False,\n    local_rank= 1,\n    load_best_model_at_end=True,\n    save_total_limit=2,\n    save_strategy=\"no\",\n    report_to=None\n)\n\nmax_train_samples = None\nmax_eval_samples=None\nmax_predict_samples=None\nmax_seq_length = 512\nbatch_size = 16\ntransformers.utils.logging.set_verbosity_info()\n\nlog_level = training_args.get_process_log_level()\nlogger.setLevel(log_level)\ndatasets.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.enable_default_handler()\ntransformers.utils.logging.enable_explicit_format()\nlogger.warning(\n    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n)\nlogger.info(f\"Training/evaluation parameters {training_args}\")\nmodel_name = 'csebuetnlp/banglabert'\nset_seed(training_args.seed)\nhate_type_map = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\nseverity_map = {'Little to None': 0, 'Mild': 1, 'Severe': 2}\nto_whom_map = {'None': 0, 'Individual': 1, 'Organization': 2, 'Community': 3, 'Society': 4}\nid2hate = {v: k for k, v in hate_type_map.items()}\nid2sev = {v: k for k, v in severity_map.items()}\nid2to = {v: k for k, v in to_whom_map.items()}\n\n# Load training and validation data\ntrain_df = pd.read_csv(train_file, sep='\\t')\n\ntrain_df['hate_type'] = train_df['hate_type'].fillna('None')\ntrain_df['to_whom'] = train_df['to_whom'].fillna('None')\ntrain_df['hate_type'] = train_df['hate_type'].map(hate_type_map).astype(int)\ntrain_df['hate_severity'] = train_df['hate_severity'].map(severity_map).astype(int)\ntrain_df['to_whom'] = train_df['to_whom'].map(to_whom_map).astype(int)\n\nvalidation_df = pd.read_csv(validation_file, sep='\\t')\nvalidation_df['hate_type'] = validation_df['hate_type'].replace('nan', 'None').fillna('None')\nvalidation_df['to_whom'] = validation_df['to_whom'].replace('nan', 'None').fillna('None')\nvalidation_df['hate_type'] = validation_df['hate_type'].map(hate_type_map).astype(int)\nvalidation_df['hate_severity'] = validation_df['hate_severity'].map(severity_map).astype(int)\nvalidation_df['to_whom'] = validation_df['to_whom'].map(to_whom_map).astype(int)\n\n# # Combine training and validation data for cross-validation\ncombined_df = pd.concat([train_df, validation_df], ignore_index=True)\ncombined_dataset = Dataset.from_pandas(combined_df)\n\n# # Load test data separately\ntest_df = pd.read_csv(test_file, sep='\\t')\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Create initial dataset dict for preprocessing\nraw_datasets = DatasetDict({\n    \"combined\": combined_dataset,\n    \"test\": test_dataset\n})\n\nfor key in raw_datasets.keys():\n    logger.info(f\"loading a local file for {key}\")\n    \nprint(f\"Combined dataset size: {len(combined_dataset)}\")\nprint(f\"Test dataset size: {len(test_df)}\")\nlen(test_df['id'])\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    cache_dir=None,\n    use_fast=True,\n    revision=\"main\",\n    use_auth_token=None,\n)\n\nclass MultiTaskModel(torch.nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.base_model = AutoModel.from_pretrained(model_name)\n        self.config = AutoConfig.from_pretrained(model_name)\n        hidden_size = self.config.hidden_size\n        self.hate_type_head = torch.nn.Linear(hidden_size, len(hate_type_map))\n        self.severity_head = torch.nn.Linear(hidden_size, len(severity_map))\n        self.to_whom_head = torch.nn.Linear(hidden_size, len(to_whom_map))\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        hate_type_logits = self.hate_type_head(pooled_output)\n        severity_logits = self.severity_head(pooled_output)\n        to_whom_logits = self.to_whom_head(pooled_output)\n        loss = None\n        if labels is not None:\n            hate_type_labels = labels[:, 0]\n            severity_labels = labels[:, 1]\n            to_whom_labels = labels[:, 2]\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(hate_type_logits, hate_type_labels.long()) + \\\n                   loss_fct(severity_logits, severity_labels.long()) + \\\n                   loss_fct(to_whom_logits, to_whom_labels.long())\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=(hate_type_logits, severity_logits, to_whom_logits),\n        )\n\nnon_label_column_names = [name for name in raw_datasets[\"combined\"].column_names if name != \"labels\"]\nsentence1_key= 'text'\n\n# Padding strategy\npadding = \"max_length\"\n\nif max_seq_length > tokenizer.model_max_length:\n    logger.warning(\n        f\"The max_seq_length passed ({max_seq_length}) is larger than the maximum length for the\"\n        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n    )\nmax_seq_length = min(max_seq_length, tokenizer.model_max_length)\n\ndef preprocess_function(examples):\n    # Tokenize the texts\n    args = (\n        (examples[sentence1_key],))\n    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n    if \"hate_type\" in examples:\n        result[\"labels\"] = [[l1, l2, l3] for l1, l2, l3 in zip(examples[\"hate_type\"], examples[\"hate_severity\"], examples[\"to_whom\"])]\n    return result\n\n# Preprocess the datasets\nraw_datasets = raw_datasets.map(\n    preprocess_function,\n    batched=True,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on dataset\",\n)\n# Prepare combined dataset for cross-validation\ncombined_dataset = raw_datasets[\"combined\"]\npredict_dataset = raw_datasets[\"test\"]\n\n# Extract features and labels for StratifiedKFold\n# For stratification, use hate_type_label as primary, since it has more classes\nX = np.arange(len(combined_dataset))  # Dummy, since we select indices\ny = np.array(combined_dataset[\"hate_type\"])  # Stratify on hate_type\n\nprint(f\"Total samples for cross-validation: {len(y)}\")\nprint(f\"Label distribution: {np.bincount(y)}\")\n\n# Initialize StratifiedKFold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Store results for each fold\nfold_results = []\nfold_probs = []\n# Cross-validation loop\naccuracy = evaluate.load(\"accuracy\")\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ndef compute_metrics(p: EvalPrediction):\n    hate_preds = np.argmax(p.predictions[0], axis=1)\n    sev_preds = np.argmax(p.predictions[1], axis=1)\n    to_preds = np.argmax(p.predictions[2], axis=1)\n    hate_labels = p.label_ids[:,0]\n    sev_labels = p.label_ids[:,1]\n    to_labels = p.label_ids[:,2]\n    hate_acc = accuracy.compute(predictions=hate_preds, references=hate_labels)['accuracy']\n    sev_acc = accuracy.compute(predictions=sev_preds, references=sev_labels)['accuracy']\n    to_acc = accuracy.compute(predictions=to_preds, references=to_labels)['accuracy']\n    return {'hate_accuracy': hate_acc, 'severity_accuracy': sev_acc, 'to_whom_accuracy': to_acc}\n\nclass FGM():\n    def __init__(self, model):\n        self.model = model\n        self.backup = {}\n\n    def attack(self, epsilon=1.0, emb_name='electra.embeddings.word_embeddings'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0 and not torch.isnan(norm):\n                    r_at = epsilon * param.grad / norm\n                    param.data.add_(r_at)\n\n    def restore(self, emb_name='electra.embeddings.word_embeddings'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                if name in self.backup:\n                    param.data = self.backup[name]\n        self.backup = {}\n\nclass CustomTrainer(Trainer):\n    def training_step(self, model: torch.nn.Module, inputs: dict, num_items_in_batch: Optional[int] = None) -> torch.Tensor:\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        # Normal forward and backward\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.gradient_accumulation_steps > 1:\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()\n\n        self.accelerator.backward(loss)\n\n        # FGM adversarial step\n        fgm = FGM(model)\n        fgm.attack(epsilon=1.0, emb_name='electra.embeddings.word_embeddings')\n\n        with self.compute_loss_context_manager():\n            loss_adv = self.compute_loss(model, inputs)\n\n        if self.args.gradient_accumulation_steps > 1:\n            loss_adv = loss_adv / self.args.gradient_accumulation_steps\n\n        if self.args.n_gpu > 1:\n            loss_adv = loss_adv.mean()\n\n        self.accelerator.backward(loss_adv)\n\n        fgm.restore(emb_name='electra.embeddings.word_embeddings')\n\n        return (loss.detach() + loss_adv.detach()) / 2\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"\\n{'='*50}\")\n    print(f\"FOLD {fold + 1}/{n_splits}\")\n    print(f\"{'='*50}\")\n    \n    # Create train and validation datasets for this fold\n    train_dataset = combined_dataset.select(train_idx.tolist())\n    val_dataset = combined_dataset.select(val_idx.tolist())\n    \n    # Remove ID columns\n    train_dataset = train_dataset.remove_columns(\"id\") if \"id\" in train_dataset.column_names else train_dataset\n    val_dataset = val_dataset.remove_columns(\"id\") if \"id\" in val_dataset.column_names else val_dataset\n    \n    print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n    \n    # Initialize model for this fold (fresh model each time)\n    model = MultiTaskModel(model_name)\n    \n    # Update training arguments for this fold\n    fold_training_args = TrainingArguments(\n        learning_rate=2e-5,\n        num_train_epochs=1,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        output_dir=f\"./distilBERT_fold_{fold+1}/\",\n        overwrite_output_dir=True,\n        remove_unused_columns=True,  # Changed to True to fix the tensor conversion error\n        local_rank=1,\n        load_best_model_at_end=True,\n        save_total_limit=1,\n        save_strategy=\"epoch\",\n        eval_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        report_to=None,\n        seed=42 + fold  # Different seed for each fold\n    )\n    \n    # Initialize trainer for this fold\n    trainer = CustomTrainer(\n        model=model,\n        args=fold_training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Train the model\n    print(f\"Training fold {fold + 1}...\")\n    train_result = trainer.train()\n    \n    # Evaluate the model\n    print(f\"Evaluating fold {fold + 1}...\")\n    eval_result = trainer.evaluate()\n    \n    # Store results\n    fold_results.append({\n        'fold': fold + 1,\n        'train_loss': train_result.metrics['train_loss'],\n        'eval_loss': eval_result['eval_loss'],\n        'eval_hate_accuracy': eval_result['eval_hate_accuracy'],\n        'eval_severity_accuracy': eval_result['eval_severity_accuracy'],\n        'eval_to_whom_accuracy': eval_result['eval_to_whom_accuracy']\n    })\n    \n    # Generate predictions on test set for this fold\n    print(f\"Predicting with fold {fold + 1} model...\")\n    test_predictions = trainer.predict(predict_dataset.remove_columns(\"id\") if \"id\" in predict_dataset.column_names else predict_dataset)\n    probs = [torch.softmax(torch.tensor(logits), dim=-1).numpy() for logits in test_predictions.predictions]\n    fold_probs.append(probs)\n    \n    # Clean up to save memory\n    del model, trainer\n    \n    print(f\"Fold {fold + 1} - Hate Accuracy: {eval_result['eval_hate_accuracy']:.4f}, Severity Accuracy: {eval_result['eval_severity_accuracy']:.4f}, To Whom Accuracy: {eval_result['eval_to_whom_accuracy']:.4f}\")\n    \nprint(f\"\\n{'='*50}\")\nprint(\"CROSS-VALIDATION COMPLETED\")\nprint(f\"{'='*50}\")\n# Analyze cross-validation results\nimport pandas as pd\n\nresults_df = pd.DataFrame(fold_results)\nprint(\"\\nCross-Validation Results:\")\nprint(results_df)\n\n# Calculate average performance metrics\navg_train_loss = results_df['train_loss'].mean()\navg_eval_loss = results_df['eval_loss'].mean()\navg_hate_acc = results_df['eval_hate_accuracy'].mean()\navg_sev_acc = results_df['eval_severity_accuracy'].mean()\navg_to_acc = results_df['eval_to_whom_accuracy'].mean()\nstd_hate_acc = results_df['eval_hate_accuracy'].std()\nstd_sev_acc = results_df['eval_severity_accuracy'].std()\nstd_to_acc = results_df['eval_to_whom_accuracy'].std()\n\nprint(f\"\\nAverage Results Across {n_splits} Folds:\")\nprint(f\"Average Training Loss: {avg_train_loss:.4f}\")\nprint(f\"Average Validation Loss: {avg_eval_loss:.4f}\")\nprint(f\"Average Hate Type Accuracy: {avg_hate_acc:.4f} ± {std_hate_acc:.4f}\")\nprint(f\"Average Severity Accuracy: {avg_sev_acc:.4f} ± {std_sev_acc:.4f}\")\nprint(f\"Average To Whom Accuracy: {avg_to_acc:.4f} ± {std_to_acc:.4f}\")\n\n# Ensemble predictions\nhate_probs_folds = np.array([probs[0] for probs in fold_probs])\nsev_probs_folds = np.array([probs[1] for probs in fold_probs])\nto_probs_folds = np.array([probs[2] for probs in fold_probs])\nensemble_probs = (np.mean(hate_probs_folds, axis=0),\n                  np.mean(sev_probs_folds, axis=0),\n                  np.mean(to_probs_folds, axis=0))\n# np.save('ensemble_probs_aug20.npy', np.array(ensemble_probs, dtype=object))\n# Final ensemble prediction\nhate_probs, sev_probs, to_probs = ensemble_probs\nfinal_hate_preds = np.argmax(hate_probs, axis=1)\nfinal_sev_preds = np.argmax(sev_probs, axis=1)\nfinal_to_preds = np.argmax(to_probs, axis=1)\n\n# Generate predictions\n\n\n# # Also save the ensemble predictions with different format for comparison\n# submission_df = pd.DataFrame({'id': test_df['id'], 'hate_type': [id2hate[p] for p in final_hate_preds], 'hate_severity': [id2sev[p] for p in final_sev_preds], 'to_whom': [id2to[p] for p in final_to_preds]})\n# submission_df.to_csv('ensemble_submission.tsv', sep='\\t', index=False)\n\n# print(\"Ensemble predictions also saved to 'ensemble_submission.tsv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:58:38.508507Z","iopub.execute_input":"2025-08-25T05:58:38.508832Z","iopub.status.idle":"2025-08-25T11:05:33.197879Z","shell.execute_reply.started":"2025-08-25T05:58:38.508810Z","shell.execute_reply":"2025-08-25T11:05:33.196752Z"}},"outputs":[{"name":"stdout","text":"--2025-08-25 05:58:38--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_dev.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 634005 (619K) [text/plain]\nSaving to: ‘blp25_hatespeech_subtask_1C_dev.tsv’\n\nblp25_hatespeech_su 100%[===================>] 619.15K  --.-KB/s    in 0.04s   \n\n2025-08-25 05:58:39 (14.4 MB/s) - ‘blp25_hatespeech_subtask_1C_dev.tsv’ saved [634005/634005]\n\n--2025-08-25 05:58:39--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_dev_test.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 548258 (535K) [text/plain]\nSaving to: ‘blp25_hatespeech_subtask_1C_dev_test.tsv’\n\nblp25_hatespeech_su 100%[===================>] 535.41K  --.-KB/s    in 0.04s   \n\n2025-08-25 05:58:39 (12.7 MB/s) - ‘blp25_hatespeech_subtask_1C_dev_test.tsv’ saved [548258/548258]\n\n--2025-08-25 05:58:39--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_train.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8686966 (8.3M) [application/octet-stream]\nSaving to: ‘blp25_hatespeech_subtask_1C_train.tsv’\n\nblp25_hatespeech_su 100%[===================>]   8.28M  --.-KB/s    in 0.09s   \n\n2025-08-25 05:58:39 (92.3 MB/s) - ‘blp25_hatespeech_subtask_1C_train.tsv’ saved [8686966/8686966]\n\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\nCollecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.5\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"2025-08-25 06:00:23.821911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756101624.120255      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756101624.197449      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Combined dataset size: 38034\nTest dataset size: 2512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50604de6edaf4c969eecab3c703db1f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc74f3758444b7e8f291021e9538247"}},"metadata":{}},{"name":"stderr","text":"[INFO|configuration_utils.py:698] 2025-08-25 06:00:45,000 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 06:00:45,009 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edfc04caea5349f195cb8cd7c68cd704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaffb3be251e4c858de91c7f1ae69888"}},"metadata":{}},{"name":"stderr","text":"[INFO|tokenization_utils_base.py:2023] 2025-08-25 06:00:45,392 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/vocab.txt\n[INFO|tokenization_utils_base.py:2023] 2025-08-25 06:00:45,393 >> loading file tokenizer.json from cache at None\n[INFO|tokenization_utils_base.py:2023] 2025-08-25 06:00:45,394 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2023] 2025-08-25 06:00:45,394 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2023] 2025-08-25 06:00:45,395 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2023] 2025-08-25 06:00:45,396 >> loading file chat_template.jinja from cache at None\n[INFO|configuration_utils.py:698] 2025-08-25 06:00:45,398 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 06:00:45,399 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|configuration_utils.py:698] 2025-08-25 06:00:45,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 06:00:45,450 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/38034 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c0de789165e45f59ee399cfc20fd985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be79c7724714c819da388240e959790"}},"metadata":{}},{"name":"stdout","text":"Total samples for cross-validation: 38034\nLabel distribution: [21405   714   133  4518  2488  8776]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3dbab0e8aea4537a532f7c34538b8cf"}},"metadata":{}},{"name":"stderr","text":"[INFO|configuration_utils.py:698] 2025-08-25 06:01:00,114 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 06:01:00,116 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nFOLD 1/5\n==================================================\nTrain size: 30427, Validation size: 7607\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725f9b955a814708bbb60a18aaf047e8"}},"metadata":{}},{"name":"stderr","text":"[INFO|modeling_utils.py:1151] 2025-08-25 06:01:08,344 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/pytorch_model.bin\n[INFO|safetensors_conversion.py:61] 2025-08-25 06:01:08,414 >> Attempting to create safetensors variant\n[INFO|safetensors_conversion.py:74] 2025-08-25 06:01:08,592 >> Safetensors PR exists\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc7eace886047f3bb06965e4bb7d7e0"}},"metadata":{}},{"name":"stderr","text":"[INFO|modeling_utils.py:5121] 2025-08-25 06:01:09,119 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[INFO|modeling_utils.py:5139] 2025-08-25 06:01:09,120 >> All the weights of ElectraModel were initialized from the model checkpoint at csebuetnlp/banglabert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n[INFO|configuration_utils.py:698] 2025-08-25 06:01:09,188 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 06:01:09,190 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|training_args.py:2135] 2025-08-25 06:01:09,194 >> PyTorch: setting up devices\n[INFO|training_args.py:1812] 2025-08-25 06:01:09,224 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n[WARNING|integration_utils.py:101] 2025-08-25 06:01:09,226 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3290950295.py:330: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomTrainer(\n[INFO|trainer.py:934] 2025-08-25 06:01:11,371 >> The following columns in the Training set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:2409] 2025-08-25 06:01:11,407 >> ***** Running training *****\n[INFO|trainer.py:2410] 2025-08-25 06:01:11,408 >>   Num examples = 30,427\n[INFO|trainer.py:2411] 2025-08-25 06:01:11,408 >>   Num Epochs = 1\n[INFO|trainer.py:2412] 2025-08-25 06:01:11,409 >>   Instantaneous batch size per device = 16\n[INFO|trainer.py:2414] 2025-08-25 06:01:11,409 >>   Training with DataParallel so batch size has been adjusted to: 32\n[INFO|trainer.py:2415] 2025-08-25 06:01:11,410 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:2416] 2025-08-25 06:01:11,411 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2417] 2025-08-25 06:01:11,411 >>   Total optimization steps = 951\n[INFO|trainer.py:2418] 2025-08-25 06:01:11,413 >>   Number of trainable parameters = 110,037,518\n","output_type":"stream"},{"name":"stdout","text":"Training fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='951' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [951/951 56:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hate Accuracy</th>\n      <th>Severity Accuracy</th>\n      <th>To Whom Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.428000</td>\n      <td>2.164515</td>\n      <td>0.700145</td>\n      <td>0.742211</td>\n      <td>0.714211</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 06:55:04,702 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 06:55:04,708 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 06:55:04,708 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 06:55:04,709 >>   Batch size = 32\n[INFO|trainer.py:3993] 2025-08-25 06:57:52,502 >> Saving model checkpoint to ./distilBERT_fold_1/checkpoint-951\n[INFO|trainer.py:4007] 2025-08-25 06:57:52,505 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2525] 2025-08-25 06:57:53,418 >> tokenizer config file saved in ./distilBERT_fold_1/checkpoint-951/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2534] 2025-08-25 06:57:53,419 >> Special tokens file saved in ./distilBERT_fold_1/checkpoint-951/special_tokens_map.json\n[INFO|trainer.py:2676] 2025-08-25 06:57:54,826 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n[INFO|trainer.py:2907] 2025-08-25 06:57:54,827 >> Loading best model from ./distilBERT_fold_1/checkpoint-951 (score: 2.1645150184631348).\n[INFO|trainer.py:934] 2025-08-25 06:57:54,995 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 06:57:55,000 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 06:57:55,001 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 06:57:55,001 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 07:00:42,539 >> The following columns in the test set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 07:00:42,544 >> \n***** Running Prediction *****\n[INFO|trainer.py:4329] 2025-08-25 07:00:42,544 >>   Num examples = 2512\n[INFO|trainer.py:4332] 2025-08-25 07:00:42,545 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Predicting with fold 1 model...\nFold 1 - Hate Accuracy: 0.7001, Severity Accuracy: 0.7422, To Whom Accuracy: 0.7142\n\n==================================================\nFOLD 2/5\n==================================================\nTrain size: 30427, Validation size: 7607\n","output_type":"stream"},{"name":"stderr","text":"[INFO|configuration_utils.py:698] 2025-08-25 07:01:37,914 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 07:01:37,915 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|modeling_utils.py:1151] 2025-08-25 07:01:37,957 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/pytorch_model.bin\n[INFO|safetensors_conversion.py:61] 2025-08-25 07:01:38,115 >> Attempting to create safetensors variant\n[INFO|safetensors_conversion.py:74] 2025-08-25 07:01:38,273 >> Safetensors PR exists\n[INFO|modeling_utils.py:5121] 2025-08-25 07:01:38,596 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[INFO|modeling_utils.py:5139] 2025-08-25 07:01:38,597 >> All the weights of ElectraModel were initialized from the model checkpoint at csebuetnlp/banglabert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n[INFO|configuration_utils.py:698] 2025-08-25 07:01:38,665 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 07:01:38,666 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|training_args.py:2135] 2025-08-25 07:01:38,669 >> PyTorch: setting up devices\n[INFO|training_args.py:1812] 2025-08-25 07:01:38,699 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n[WARNING|integration_utils.py:101] 2025-08-25 07:01:38,702 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3290950295.py:330: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training fold 2...\n","output_type":"stream"},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 07:01:39,702 >> The following columns in the Training set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:2409] 2025-08-25 07:01:39,710 >> ***** Running training *****\n[INFO|trainer.py:2410] 2025-08-25 07:01:39,711 >>   Num examples = 30,427\n[INFO|trainer.py:2411] 2025-08-25 07:01:39,711 >>   Num Epochs = 1\n[INFO|trainer.py:2412] 2025-08-25 07:01:39,712 >>   Instantaneous batch size per device = 16\n[INFO|trainer.py:2414] 2025-08-25 07:01:39,712 >>   Training with DataParallel so batch size has been adjusted to: 32\n[INFO|trainer.py:2415] 2025-08-25 07:01:39,713 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:2416] 2025-08-25 07:01:39,714 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2417] 2025-08-25 07:01:39,714 >>   Total optimization steps = 951\n[INFO|trainer.py:2418] 2025-08-25 07:01:39,716 >>   Number of trainable parameters = 110,037,518\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='951' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [951/951 56:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hate Accuracy</th>\n      <th>Severity Accuracy</th>\n      <th>To Whom Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.449100</td>\n      <td>2.180907</td>\n      <td>0.704746</td>\n      <td>0.736953</td>\n      <td>0.706849</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 07:55:48,833 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 07:55:48,839 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 07:55:48,839 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 07:55:48,840 >>   Batch size = 32\n[INFO|trainer.py:3993] 2025-08-25 07:58:36,638 >> Saving model checkpoint to ./distilBERT_fold_2/checkpoint-951\n[INFO|trainer.py:4007] 2025-08-25 07:58:36,641 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2525] 2025-08-25 07:58:37,471 >> tokenizer config file saved in ./distilBERT_fold_2/checkpoint-951/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2534] 2025-08-25 07:58:37,473 >> Special tokens file saved in ./distilBERT_fold_2/checkpoint-951/special_tokens_map.json\n[INFO|trainer.py:2676] 2025-08-25 07:58:38,848 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n[INFO|trainer.py:2907] 2025-08-25 07:58:38,848 >> Loading best model from ./distilBERT_fold_2/checkpoint-951 (score: 2.1809067726135254).\n[INFO|trainer.py:934] 2025-08-25 07:58:39,018 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 07:58:39,023 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 07:58:39,024 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 07:58:39,024 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 08:01:26,983 >> The following columns in the test set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 08:01:26,988 >> \n***** Running Prediction *****\n[INFO|trainer.py:4329] 2025-08-25 08:01:26,989 >>   Num examples = 2512\n[INFO|trainer.py:4332] 2025-08-25 08:01:26,990 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Predicting with fold 2 model...\nFold 2 - Hate Accuracy: 0.7047, Severity Accuracy: 0.7370, To Whom Accuracy: 0.7068\n\n==================================================\nFOLD 3/5\n==================================================\nTrain size: 30427, Validation size: 7607\n","output_type":"stream"},{"name":"stderr","text":"[INFO|configuration_utils.py:698] 2025-08-25 08:02:22,841 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 08:02:22,842 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|modeling_utils.py:1151] 2025-08-25 08:02:22,884 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/pytorch_model.bin\n[INFO|safetensors_conversion.py:61] 2025-08-25 08:02:22,968 >> Attempting to create safetensors variant\n[INFO|safetensors_conversion.py:74] 2025-08-25 08:02:23,123 >> Safetensors PR exists\n[INFO|modeling_utils.py:5121] 2025-08-25 08:02:23,529 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[INFO|modeling_utils.py:5139] 2025-08-25 08:02:23,530 >> All the weights of ElectraModel were initialized from the model checkpoint at csebuetnlp/banglabert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n[INFO|configuration_utils.py:698] 2025-08-25 08:02:23,597 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 08:02:23,599 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|training_args.py:2135] 2025-08-25 08:02:23,601 >> PyTorch: setting up devices\n[INFO|training_args.py:1812] 2025-08-25 08:02:23,650 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n[WARNING|integration_utils.py:101] 2025-08-25 08:02:23,653 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3290950295.py:330: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training fold 3...\n","output_type":"stream"},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 08:02:24,696 >> The following columns in the Training set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:2409] 2025-08-25 08:02:24,703 >> ***** Running training *****\n[INFO|trainer.py:2410] 2025-08-25 08:02:24,704 >>   Num examples = 30,427\n[INFO|trainer.py:2411] 2025-08-25 08:02:24,704 >>   Num Epochs = 1\n[INFO|trainer.py:2412] 2025-08-25 08:02:24,705 >>   Instantaneous batch size per device = 16\n[INFO|trainer.py:2414] 2025-08-25 08:02:24,706 >>   Training with DataParallel so batch size has been adjusted to: 32\n[INFO|trainer.py:2415] 2025-08-25 08:02:24,706 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:2416] 2025-08-25 08:02:24,707 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2417] 2025-08-25 08:02:24,707 >>   Total optimization steps = 951\n[INFO|trainer.py:2418] 2025-08-25 08:02:24,708 >>   Number of trainable parameters = 110,037,518\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='951' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [951/951 57:04, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hate Accuracy</th>\n      <th>Severity Accuracy</th>\n      <th>To Whom Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.417200</td>\n      <td>2.201315</td>\n      <td>0.696464</td>\n      <td>0.731563</td>\n      <td>0.702642</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 08:56:40,490 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 08:56:40,495 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 08:56:40,495 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 08:56:40,496 >>   Batch size = 32\n[INFO|trainer.py:3993] 2025-08-25 08:59:30,023 >> Saving model checkpoint to ./distilBERT_fold_3/checkpoint-951\n[INFO|trainer.py:4007] 2025-08-25 08:59:30,027 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2525] 2025-08-25 08:59:30,872 >> tokenizer config file saved in ./distilBERT_fold_3/checkpoint-951/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2534] 2025-08-25 08:59:30,873 >> Special tokens file saved in ./distilBERT_fold_3/checkpoint-951/special_tokens_map.json\n[INFO|trainer.py:2676] 2025-08-25 08:59:32,171 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n[INFO|trainer.py:2907] 2025-08-25 08:59:32,172 >> Loading best model from ./distilBERT_fold_3/checkpoint-951 (score: 2.201315402984619).\n[INFO|trainer.py:934] 2025-08-25 08:59:32,336 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 08:59:32,340 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 08:59:32,341 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 08:59:32,341 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 09:02:22,244 >> The following columns in the test set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 09:02:22,248 >> \n***** Running Prediction *****\n[INFO|trainer.py:4329] 2025-08-25 09:02:22,249 >>   Num examples = 2512\n[INFO|trainer.py:4332] 2025-08-25 09:02:22,249 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Predicting with fold 3 model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO|configuration_utils.py:698] 2025-08-25 09:03:18,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 09:03:18,244 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Hate Accuracy: 0.6965, Severity Accuracy: 0.7316, To Whom Accuracy: 0.7026\n\n==================================================\nFOLD 4/5\n==================================================\nTrain size: 30427, Validation size: 7607\n","output_type":"stream"},{"name":"stderr","text":"[INFO|modeling_utils.py:1151] 2025-08-25 09:03:18,286 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/pytorch_model.bin\n[INFO|safetensors_conversion.py:61] 2025-08-25 09:03:18,378 >> Attempting to create safetensors variant\n[INFO|safetensors_conversion.py:74] 2025-08-25 09:03:18,538 >> Safetensors PR exists\n[INFO|modeling_utils.py:5121] 2025-08-25 09:03:18,886 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[INFO|modeling_utils.py:5139] 2025-08-25 09:03:18,887 >> All the weights of ElectraModel were initialized from the model checkpoint at csebuetnlp/banglabert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n[INFO|configuration_utils.py:698] 2025-08-25 09:03:18,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 09:03:18,942 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|training_args.py:2135] 2025-08-25 09:03:18,944 >> PyTorch: setting up devices\n[INFO|training_args.py:1812] 2025-08-25 09:03:18,974 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n[WARNING|integration_utils.py:101] 2025-08-25 09:03:18,975 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3290950295.py:330: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training fold 4...\n","output_type":"stream"},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 09:03:19,945 >> The following columns in the Training set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:2409] 2025-08-25 09:03:19,952 >> ***** Running training *****\n[INFO|trainer.py:2410] 2025-08-25 09:03:19,953 >>   Num examples = 30,427\n[INFO|trainer.py:2411] 2025-08-25 09:03:19,954 >>   Num Epochs = 1\n[INFO|trainer.py:2412] 2025-08-25 09:03:19,954 >>   Instantaneous batch size per device = 16\n[INFO|trainer.py:2414] 2025-08-25 09:03:19,955 >>   Training with DataParallel so batch size has been adjusted to: 32\n[INFO|trainer.py:2415] 2025-08-25 09:03:19,955 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:2416] 2025-08-25 09:03:19,956 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2417] 2025-08-25 09:03:19,957 >>   Total optimization steps = 951\n[INFO|trainer.py:2418] 2025-08-25 09:03:19,958 >>   Number of trainable parameters = 110,037,518\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='951' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [951/951 57:13, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hate Accuracy</th>\n      <th>Severity Accuracy</th>\n      <th>To Whom Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.446400</td>\n      <td>2.189182</td>\n      <td>0.708821</td>\n      <td>0.743263</td>\n      <td>0.704614</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 09:57:45,471 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 09:57:45,476 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 09:57:45,477 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 09:57:45,478 >>   Batch size = 32\n[INFO|trainer.py:3993] 2025-08-25 10:00:34,819 >> Saving model checkpoint to ./distilBERT_fold_4/checkpoint-951\n[INFO|trainer.py:4007] 2025-08-25 10:00:34,822 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2525] 2025-08-25 10:00:35,722 >> tokenizer config file saved in ./distilBERT_fold_4/checkpoint-951/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2534] 2025-08-25 10:00:35,722 >> Special tokens file saved in ./distilBERT_fold_4/checkpoint-951/special_tokens_map.json\n[INFO|trainer.py:2676] 2025-08-25 10:00:37,184 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n[INFO|trainer.py:2907] 2025-08-25 10:00:37,185 >> Loading best model from ./distilBERT_fold_4/checkpoint-951 (score: 2.1891818046569824).\n[INFO|trainer.py:934] 2025-08-25 10:00:37,361 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 10:00:37,366 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 10:00:37,367 >>   Num examples = 7607\n[INFO|trainer.py:4332] 2025-08-25 10:00:37,368 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 10:03:27,820 >> The following columns in the test set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 10:03:27,824 >> \n***** Running Prediction *****\n[INFO|trainer.py:4329] 2025-08-25 10:03:27,825 >>   Num examples = 2512\n[INFO|trainer.py:4332] 2025-08-25 10:03:27,825 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Predicting with fold 4 model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO|configuration_utils.py:698] 2025-08-25 10:04:24,262 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 10:04:24,263 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Hate Accuracy: 0.7088, Severity Accuracy: 0.7433, To Whom Accuracy: 0.7046\n\n==================================================\nFOLD 5/5\n==================================================\nTrain size: 30428, Validation size: 7606\n","output_type":"stream"},{"name":"stderr","text":"[INFO|modeling_utils.py:1151] 2025-08-25 10:04:24,306 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/pytorch_model.bin\n[INFO|safetensors_conversion.py:61] 2025-08-25 10:04:24,371 >> Attempting to create safetensors variant\n[INFO|safetensors_conversion.py:74] 2025-08-25 10:04:24,542 >> Safetensors PR exists\n[INFO|modeling_utils.py:5121] 2025-08-25 10:04:25,019 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[INFO|modeling_utils.py:5139] 2025-08-25 10:04:25,020 >> All the weights of ElectraModel were initialized from the model checkpoint at csebuetnlp/banglabert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n[INFO|configuration_utils.py:698] 2025-08-25 10:04:25,082 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:770] 2025-08-25 10:04:25,083 >> Model config ElectraConfig {\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|training_args.py:2135] 2025-08-25 10:04:25,085 >> PyTorch: setting up devices\n[INFO|training_args.py:1812] 2025-08-25 10:04:25,117 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n[WARNING|integration_utils.py:101] 2025-08-25 10:04:25,119 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3290950295.py:330: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training fold 5...\n","output_type":"stream"},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 10:04:26,159 >> The following columns in the Training set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:2409] 2025-08-25 10:04:26,168 >> ***** Running training *****\n[INFO|trainer.py:2410] 2025-08-25 10:04:26,168 >>   Num examples = 30,428\n[INFO|trainer.py:2411] 2025-08-25 10:04:26,169 >>   Num Epochs = 1\n[INFO|trainer.py:2412] 2025-08-25 10:04:26,170 >>   Instantaneous batch size per device = 16\n[INFO|trainer.py:2414] 2025-08-25 10:04:26,170 >>   Training with DataParallel so batch size has been adjusted to: 32\n[INFO|trainer.py:2415] 2025-08-25 10:04:26,171 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:2416] 2025-08-25 10:04:26,171 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2417] 2025-08-25 10:04:26,172 >>   Total optimization steps = 951\n[INFO|trainer.py:2418] 2025-08-25 10:04:26,174 >>   Number of trainable parameters = 110,037,518\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='951' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [951/951 57:16, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hate Accuracy</th>\n      <th>Severity Accuracy</th>\n      <th>To Whom Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.429300</td>\n      <td>2.161227</td>\n      <td>0.708125</td>\n      <td>0.746779</td>\n      <td>0.703129</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 10:58:53,942 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 10:58:53,948 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 10:58:53,948 >>   Num examples = 7606\n[INFO|trainer.py:4332] 2025-08-25 10:58:53,949 >>   Batch size = 32\n[INFO|trainer.py:3993] 2025-08-25 11:01:43,864 >> Saving model checkpoint to ./distilBERT_fold_5/checkpoint-951\n[INFO|trainer.py:4007] 2025-08-25 11:01:43,868 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n[INFO|tokenization_utils_base.py:2525] 2025-08-25 11:01:44,853 >> tokenizer config file saved in ./distilBERT_fold_5/checkpoint-951/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2534] 2025-08-25 11:01:44,854 >> Special tokens file saved in ./distilBERT_fold_5/checkpoint-951/special_tokens_map.json\n[INFO|trainer.py:2676] 2025-08-25 11:01:46,346 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n[INFO|trainer.py:2907] 2025-08-25 11:01:46,347 >> Loading best model from ./distilBERT_fold_5/checkpoint-951 (score: 2.1612274646759033).\n[INFO|trainer.py:934] 2025-08-25 11:01:46,526 >> The following columns in the Evaluation set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: to_whom, text, hate_severity, token_type_ids, hate_type. If to_whom, text, hate_severity, token_type_ids, hate_type are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 11:01:46,531 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4329] 2025-08-25 11:01:46,532 >>   Num examples = 7606\n[INFO|trainer.py:4332] 2025-08-25 11:01:46,532 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"[INFO|trainer.py:934] 2025-08-25 11:04:36,807 >> The following columns in the test set don't have a corresponding argument in `MultiTaskModel.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `MultiTaskModel.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4327] 2025-08-25 11:04:36,812 >> \n***** Running Prediction *****\n[INFO|trainer.py:4329] 2025-08-25 11:04:36,812 >>   Num examples = 2512\n[INFO|trainer.py:4332] 2025-08-25 11:04:36,813 >>   Batch size = 32\n","output_type":"stream"},{"name":"stdout","text":"Predicting with fold 5 model...\nFold 5 - Hate Accuracy: 0.7081, Severity Accuracy: 0.7468, To Whom Accuracy: 0.7031\n\n==================================================\nCROSS-VALIDATION COMPLETED\n==================================================\n\nCross-Validation Results:\n   fold  train_loss  eval_loss  eval_hate_accuracy  eval_severity_accuracy  \\\n0     1    2.428029   2.164515            0.700145                0.742211   \n1     2    2.449084   2.180907            0.704746                0.736953   \n2     3    2.417230   2.201315            0.696464                0.731563   \n3     4    2.446381   2.189182            0.708821                0.743263   \n4     5    2.429259   2.161227            0.708125                0.746779   \n\n   eval_to_whom_accuracy  \n0               0.714211  \n1               0.706849  \n2               0.702642  \n3               0.704614  \n4               0.703129  \n\nAverage Results Across 5 Folds:\nAverage Training Loss: 2.4340\nAverage Validation Loss: 2.1794\nAverage Hate Type Accuracy: 0.7037 ± 0.0053\nAverage Severity Accuracy: 0.7402 ± 0.0060\nAverage To Whom Accuracy: 0.7063 ± 0.0047\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3290950295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;31m# Write predictions in the required format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_predict_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"***** Predict results *****\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\\thate_type\\thate_severity\\tto_whom\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './distilBERT_m/subtask_1C.tsv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './distilBERT_m/subtask_1C.tsv'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.makedirs(training_args.output_dir, exist_ok=True)\nlogger.info(\"*** Predict ***\")\nids = test_df['id']\noutput_predict_file = os.path.join(training_args.output_dir, f\"subtask_1C.tsv\")\n\n# Write predictions in the required format\nwith open(output_predict_file, \"w\") as writer:\n    logger.info(f\"***** Predict results *****\")\n    writer.write(\"id\\thate_type\\thate_severity\\tto_whom\\tmodel\\n\")\n    for index in range(len(final_hate_preds)):\n        h = id2hate[final_hate_preds[index]]\n        s = id2sev[final_sev_preds[index]]\n        t = id2to[final_to_preds[index]]\n        writer.write(f\"{ids[index]}\\t{h}\\t{s}\\t{t}\\t{model_name}\\n\")\n\nprint(f\"\\nPredictions saved to '{output_predict_file}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:35:45.369307Z","iopub.execute_input":"2025-08-25T11:35:45.370215Z","iopub.status.idle":"2025-08-25T11:35:45.387400Z","shell.execute_reply.started":"2025-08-25T11:35:45.370179Z","shell.execute_reply":"2025-08-25T11:35:45.386664Z"}},"outputs":[{"name":"stdout","text":"\nPredictions saved to './distilBERT_m/subtask_1C.tsv'\n","output_type":"stream"}],"execution_count":3}]}